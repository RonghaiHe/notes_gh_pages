{"config":{"lang":["zh","en","ja"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"control/optimal_control/optimal_control1/","title":"1 Introduction","text":"<p> \u7ea6 1532 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 6 \u5206\u949f</p> <p>Quote</p> <p>An introduction to optimal control</p>"},{"location":"control/optimal_control/optimal_control1/#11-basic-problem","title":"1.1 basic problem","text":"<p>DYNAMICS: Consider an ODE:</p> \\[\\left\\{\\begin{aligned} \\dot{\\boldsymbol{x}}(t) &amp; =\\boldsymbol{f}(\\boldsymbol{x}(t)) \\quad(t&gt;0) \\\\ \\boldsymbol{x}(0) &amp; =x^0\\end{aligned}\\right.\\] <ul> <li>\\(x^0\\in\\mathbb{R}^n\\): initial point</li> <li>\\(\\boldsymbol{f}:\\mathbb{R}^n\\to\\mathbb{R}^n\\): dynamic function</li> <li>\\(\\boldsymbol{x}: [0, \\infty)\\to\\mathbb{R}^n\\): unknown; dynamic evolution of the state of some \"system\" </li> </ul> <p>CONTROLLED DYNAMICS: Generalize a bit, \\(\\boldsymbol{f}\\) also depends upon some \"control\" parameters \\(a\\in A\\subset \\mathbb{R}^m\\). So \\(\\boldsymbol{f}: \\mathbb{R}^n\\times A\\to\\mathbb{R}^n\\): \\(\\(\\left\\{\\begin{aligned} \\dot{\\boldsymbol{x}}(t) &amp; =\\boldsymbol{f}(\\boldsymbol{x}(t), a) \\quad(t&gt;0) \\\\ \\boldsymbol{x}(0) &amp; =x^0\\end{aligned}\\right.\\)\\)</p> <p>Change the value \\(a\\) as the system evolves, like: \\(\\(\\boldsymbol{\\alpha}(t)=\\left\\{\\begin{array}{ll}a_1 &amp; 0 \\leq t \\leq t_1 \\\\ a_2 &amp; t_1&lt;t \\leq t_2 \\\\ a_3 &amp; t_2&lt;t \\leq t_3\\end{array} \\quad\\right.etc.\\)\\) </p> <p>Then the dynamic equation becomes:  \\(\\(\\left\\{\\begin{aligned} \\dot{\\boldsymbol{x}}(t) &amp; =\\boldsymbol{f}(\\boldsymbol{x}(t), \\boldsymbol{\\alpha}(t)) \\quad(t&gt;0) \\\\ \\boldsymbol{x}(0) &amp; =x^0\\end{aligned}\\right.\\)\\)</p> <p></p> <p>We call a function \\(\\boldsymbol{\\alpha}: [0, \\infty)\\to A\\) a control and regard the trajectory \\(\\boldsymbol{x}(t)\\) as te corresponding response of the system.</p>"},{"location":"control/optimal_control/optimal_control1/#notation","title":"Notation","text":"<ul> <li>Introduce:   \\(\\(\\mathcal{A}=\\{\\boldsymbol{\\alpha}: [0,\\infty) \\to A | \\boldsymbol{\\alpha}(\\cdot) \\mathrm{measurable}\\}\\)\\)   To denote the collection of all admissible controls </li> <li>\\(\\boldsymbol{x}(\\cdot)=\\boldsymbol{x}(\\cdot, \\boldsymbol{\\alpha}(\\cdot),x^0)\\) would be more precise</li> </ul>"},{"location":"control/optimal_control/optimal_control1/#payoffs","title":"Payoffs","text":"<p>Overall task will be to determine what is the \"best\" control for our system. For this we need to specify a specific payoff (or reward) criterion. Let us define the payoff functional</p> \\[P[\\boldsymbol{\\alpha}(\\cdot)]:=\\int_0^T r(\\boldsymbol{x}(t), \\boldsymbol{\\alpha}(t)) \\mathrm{d} t+g(\\boldsymbol{x}(T))\\] <p>where:</p> <ul> <li>\\(\\boldsymbol{x}(\\cdot)\\) solves ODE for the control \\(\\boldsymbol{\\alpha}(\\cdot)\\)</li> <li>\\(r: \\mathbb{R}^n\\times A\\to \\mathbb{R}\\): Given; runnig payoff</li> <li>\\(g: \\mathbb{R}^n\\to \\mathbb{R}\\): Given; terminal payoff; </li> <li>\\(T\\): Given; terminal time</li> </ul>"},{"location":"control/optimal_control/optimal_control1/#the-basic-problem","title":"The basic problem","text":"<p>Aim: find a control \\(\\boldsymbol{\\alpha}^*(\\cdot)\\) which maximizes the payoff:</p> \\[P[\\boldsymbol{\\alpha}^*(\\cdot)] \\ge P[\\boldsymbol{\\alpha}(\\cdot)]\\] <p>for all controls \\(\\boldsymbol{\\alpha}(\\cdot)\\in \\mathcal{A}\\). Such a control \\(\\boldsymbol{\\alpha}^*(\\cdot)\\) is called an optimal control.</p> <p>This task presents us with these mathematical issues:</p> <ol> <li>Does an optimal control exist?</li> <li>How can we characterize an optimal control mathematically?</li> <li>How can we construct an optimal control?</li> </ol> <p>These turn out to be sometimes subtle problems, as the following collection of examples illustrates.</p>"},{"location":"control/optimal_control/optimal_control1/#12-examples","title":"1.2 Examples","text":"<p>Example 1: Control of production and consumption</p> <p>Suppose we own, say, a factory whose output we can control. Let us begin to construct a mathematical model by setting</p> <p>\\(x(t)=\\) amount of output produced at time \\(t\\ge 0\\)</p> <p>We suppose that we consume some fraction of our output at each time, and likewise can reinvest the remaining fraction. Let us denote:</p> <p>\\(\\alpha(t)=\\) fraction of output reinvested at time \\(t\\ge 0\\)</p> <p>This will be our control, and s.t. the obvious constraint that:</p> \\[0\\le \\alpha(t)\\le 1, \\text{for each time }t\\ge 0\\] <p>The corresponding dynamic equation is:</p> \\[\\left\\{\\begin{aligned} \\dot{x}(t) &amp; =k\\alpha(t)x(t) \\quad(t&gt;0) \\\\ x(0) &amp; =x^0\\end{aligned}\\right.\\] <p>The constant \\(k&gt;0\\) modelling the growth rate of our reinvestment.</p> <p>Take as a payoff functional:</p> \\[P[\\alpha(\\cdot)]:=\\int_0^T (1-\\alpha(t))x(t) \\mathrm{d} t\\] <p>That means we want to maximize our total consumption of the output, our consumption at a given time t being \\((1-\\alpha(t))x(t)\\). Here \\(n=m=1\\) and:</p> \\[A=[0,1], \\, f(x,a)=kax, \\, r(x,a)=(1-a)x, \\, g\\equiv 0.\\] <p>In 4.4.2, we will see that the optimal control is \\(\\alpha^*\\) is given by:</p> \\[\\alpha^*=\\left\\{\\begin{array}{ll}1 &amp; 0 \\le x(t) \\le t^* \\\\ 0 &amp; t^*&lt; x(t)\\le T\\end{array}\\right.\\] <p>In other words, we should reinvest all the output (and therefore consume nothing) up until time \\(t^*\\), and afterwards, we should consume everything (and therefore reinvest nothing). The switchover time \\(t^*\\) will have to be determined. We call \\(\\alpha^*(\\cdot)\\) a bang\u2013bang control.</p> <p>Example 2: Reproduction strategies in social insects</p> <p>In this example, we consider a population of social insects, a population if bees. Write \\(T\\) for the length of the season,, and introduce the variables:</p> <ul> <li>\\(w(t)=\\) number of workers at time \\(t\\)</li> <li>\\(q(t)=\\) number of queens</li> <li>\\(\\alpha(t)=\\) fraction of colony effort devoted to increasing work force</li> </ul> <p>Constraint of \\(\\alpha(t)\\): \\(0\\le \\alpha(t)\\le 1\\)</p> <p>Introduce the dynamic for the numbers of workers and the number of queens:</p> <ul> <li>workers:    \\(\\(\\left\\{\\begin{aligned} \\dot{w}(t) &amp; =-\\mu w(t) + bs(t)\\alpha(t)w(t)\\\\ w(0) &amp; =w^0 \\end{aligned}\\right.\\)\\)</li> <li>\\(\\mu\\):is the death rate of workers; given constant</li> <li>\\(s(t)\\): the known rate at which each worker contributes to the bee economy</li> <li>queens:   \\(\\(\\left\\{\\begin{aligned} \\dot{q}(t) &amp; =-\\nu q(t) + c(1-\\alpha(t))s(t)w(t)\\\\ q(0) &amp; =q^0 \\end{aligned}\\right.\\)\\)</li> <li>\\(\\nu, c\\) constant</li> </ul> <p>Goal: maximize the queens at time \\(T\\):</p> \\[P[\\alpha(\\cdot)]=q(T)\\] <p>We have \\(\\boldsymbol{x}(t)=(w(t),q(t))^\\top\\) and \\(x^0=(w^0,q^0)^\\top\\), take \\(r\\equiv 0\\) and \\(g(w,q)=q\\)</p> <p>answer will again turn out to be a bang\u2013bang control</p> <p>Example 3: A pendulum</p> <p>A hanging pendlum: \\(\\theta(t)=\\) angle of the pendulum at time \\(t\\ge 0\\)</p> <p>If no external force:</p> \\[\\left\\{\\begin{aligned} &amp;\\ddot{\\theta}(t) +\\lambda \\dot{\\theta}(t) + \\omega^2\\theta(t) = 0 \\\\ &amp;\\theta(0)  =\\theta_1, \\, \\dot{\\theta}(0)=\\theta_2\\end{aligned}\\right.\\] <p>The solution will be a damped oscillation, provided \\(\\lambda&gt;0\\)</p> <p>Let \\(\\alpha(t)\\) denote an applied torque: \\(|\\alpha|\\le 1\\) </p> <p>Our dynamics now become</p> \\[\\begin{equation*} \\left\\{\\begin{array}{l} \\ddot{\\theta}(t)+\\lambda \\dot{\\theta}(t)+\\omega^2 \\theta(t)=\\alpha(t) \\\\ \\theta(0)=\\theta_1, \\dot{\\theta}(0)=\\theta_2 \\end{array}\\right. \\end{equation*}\\] <p>Define \\(x_1(t)=\\theta(t), x_2(t)=\\dot{\\theta}(t)\\), and \\(\\boldsymbol{x}(t)=\\left(x_1(t), x_2(t)\\right)\\). Then we can write the evolution as the system</p> \\[\\begin{equation*} \\dot{\\boldsymbol{x}}(t)=\\binom{\\dot{x}_1}{\\dot{x}_2}=\\binom{\\dot{\\theta}}{\\ddot{\\theta}}=\\binom{x_2}{-\\lambda x_2-\\omega^2 x_1+\\alpha(t)}=\\boldsymbol{f}(\\boldsymbol{x}, \\alpha) . \\end{equation*}\\] <p>We introduce as well</p> \\[\\begin{equation*} P[\\alpha(\\cdot)]=-\\int_0^\\tau 1 d t=-\\tau \\end{equation*}\\] <p>for</p> \\[\\begin{equation*} \\tau=\\tau(\\alpha(\\cdot))=\\text { first time that } \\boldsymbol{x}(\\tau)=0 \\quad \\text { (that is, } \\theta(\\tau)=\\dot{\\theta}(\\tau)=0 .) \\end{equation*}\\] <p>Maximize \\(P[\\cdot]\\), meaning that we want to minimize the time it takes to bring the pendulum to rest.</p> <p>The terminal time isn't fixed, but rather depends upon the control. This's a fixed endpoint, free time problem.</p> <p>Example 4: A moon lander This model asks us to bring a spacecraft to a soft landing on the lunar surface, using the least amount of fuel.</p> <p>Introduce the notation:</p> <ul> <li>\\(h(t)\\): height at time \\(t\\)</li> <li>\\(v(t)\\): velocity \\(=\\dot{h}(t)\\)</li> <li>\\(m(t)\\): mass of spacecraft at time \\(t\\) (changing as fuel is burned)</li> <li>\\(\\alpha(t)\\): thrust at time t, assumed that \\(0\\le \\alpha(t)\\le 1\\)</li> </ul> <p>For Newton's law:</p> \\[m\\ddot{h}=-gm+\\alpha\\] <p>Modelled by ODE:</p> \\[\\left\\{\\begin{aligned} \\dot{v}(t) &amp; =-g+\\frac{\\alpha(t)}{m(t)} \\\\ \\dot{h}(t) &amp; =v(t) \\\\ \\dot{m}(t) &amp; =-k \\alpha(t)\\end{aligned}\\right.\\] <p>We want to minimize the amount of fuel used up, that is, to maximize the amount remaining once we have landed. Thus</p> \\[P[\\alpha(\\cdot)]=m(\\tau)\\] <p>where \\(\\tau\\) is the first time that \\(h(\\tau)=v(\\tau)=0\\). This's a variable endpoint problem, since the final time is not given in advance.</p> <p>We have also the extra constraints \\(h(t)\\ge 0, \\, m(t) \\ge 0\\)</p> <p>Example 5: Rocket railroad car</p> <p>Imagine a railroad car powered by rocket engines on each side. We introduce the variables:</p> <ul> <li>\\(q(t)\\): position at time \\(t\\)</li> <li>\\(v(t)=\\dot{q}(t)\\): velocity at time \\(t\\)</li> <li>\\(\\alpha(t)\\): thrust from rockets at time \\(t\\), assumed that \\(-1 \\le \\alpha(t)\\le 1\\) </li> </ul> <p>We want to figure out how to fire the rockets, so as to arrive at the origin 0 with zero velocity in a minimum amount of time. Assuming the car has mass \\(m = 1\\), the law of motion is \\(\\(\\ddot{q}(t)=\\alpha(t)\\)\\)</p> <p>Rewrie by setting \\(\\boldsymbol{x}(t)=(q(t), v(t))^\\top\\). Then</p> \\[\\left\\{\\begin{array}{l}\\dot{\\boldsymbol{x}}(t)=\\left(\\begin{array}{ll}0 &amp; 1 \\\\ 0 &amp; 0\\end{array}\\right) \\boldsymbol{x}(t)+\\binom{0}{1} \\alpha(t) \\\\ \\boldsymbol{x}(0)=x^0=\\left(q_0, v_0\\right)^T\\end{array}\\right.\\] <p>Take</p> \\[P[\\alpha(\\cdot)]=-\\int_0^\\tau 1\\mathrm{d}t=-\\tau\\] <p>for \\(\\tau=\\) first time that \\(q(\\tau)=v(\\tau)=0\\)</p>"},{"location":"control/optimal_control/optimal_control1/#13-a-geometric-solution","title":"1.3 A geometric solution","text":"<p>Introduce some ad hoc calculus and geometry methods for the rocke car problem.  </p> <p>First of all, let us guess that to find an optimal solution we will need only to consider the cases \\(\\alpha = 1\\) or \\(\\alpha = \u22121\\). In other words, we will focus our attention only upon those controls for which at each moment of time either the left or the right rocket engine is fired at full power.</p> <ul> <li>CASE 1: \\(\\alpha \\equiv 1\\) \\(\\(\\begin{cases}     \\dot{q}=v \\\\     \\dot{v}=1   \\end{cases}\\)\\)</li> </ul> <p>Then    \\(\\(v\\dot{v}=\\dot{q}\\)\\)</p> <p>And so   \\(\\(\\frac12 (v^2)^\\cdot=\\dot{q}\\)\\)</p> <p>Let \\(t_0\\) belong to the time interval where \\(\\alpha\\equiv 1\\) and interate from \\(t_0\\) to \\(t\\):   \\(\\(\\frac{v^2(t)}{2}-\\frac{v^2(t_0)}{2}=q(t)-q(t_0)\\)\\)</p> <p>Then   \\(\\(v^2(t)=2 q(t)+\\underbrace{\\left(v^2\\left(t_0\\right)-2 q\\left(t_0\\right)\\right)}_b\\)\\)</p> <p>In other words, so long as the control is set for \\(\\alpha\\equiv 1\\), the trajectory stays on the curve \\(v^2 = 2q + b\\) for some constant \\(b\\).</p> <p></p> <ul> <li>CASE 2: \\(\\alpha \\equiv -1\\) \\(\\(\\begin{cases}     \\dot{q}=v \\\\     \\dot{v}=-1   \\end{cases}\\)\\)</li> </ul> <p>Then</p> <p>\\(\\(\\frac12 (v^2)^\\cdot=-\\dot{q}\\)\\)</p> <p>Let \\(t_1\\) belong to the time interval where \\(\\alpha\\equiv -1\\) and interate from \\(t_1\\) to \\(t\\):</p> <p>\\(\\(v^2(t)=-2 q(t)+\\underbrace{\\left(2 q\\left(t_1\\right)-v^2\\left(t_1\\right)\\right)}_c\\)\\)</p> <p>As long as the control is set for \\(\\alpha\\equiv -1\\), the trajectory stays on the curve \\(v^2 = -2q + c\\) for some constant \\(c\\).</p> <p></p>"},{"location":"control/optimal_control/optimal_control1/#geometric-interpretation","title":"Geometric interpretation","text":"<p>Now we can design an optimal control \\(\\alpha^*(\\cdot)\\), which causes the trajectory to jump between the families of right\u2013 and left\u2013pointing parabolas, as drawn. Say we start at the black dot, and wish to steer to the origin. This we accomplish by first setting the control to the value \\(\\alpha = \u22121\\), causing us to move down along the second family of parabolas. We then switch to the control \\(\\alpha = 1\\), and thereupon move to a parabola from the first family, along which we move up and to the left, ending up at the origin. See the picture.</p> <p></p>"},{"location":"control/optimal_control/optimal_control2/","title":"2 Controllablity, bang-bang control","text":"<p> \u7ea6 2625 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 10 \u5206\u949f</p>"},{"location":"control/optimal_control/optimal_control2/#21-definitions","title":"2.1 Definitions","text":""},{"location":"control/optimal_control/optimal_control2/#controllability-question","title":"Controllability question","text":"<p>Given the initial point \\(x^0\\) and a \u201ctarget\u201d set \\(S \\subset \\mathbb{R}^n\\), does there exist a control steering the system to \\(S\\) in finite time?</p> <p>For the time being we will therefore not introduce any payoff criterion that would characterize an \u201coptimal\u201d control, but instead will focus on the question as to whether or not there exist controls that steer the system to a given goal. In this chapter we will mostly consider the problem of driving the system to the origin \\(S = \\{0\\}\\).</p>"},{"location":"control/optimal_control/optimal_control2/#definition","title":"Definition","text":"<ul> <li>reachable set for time \\(t\\):   \\(\\mathcal{C}(t)=\\) set of initial points \\(x^0\\) for which there exists a control such that \\(x(t) = 0\\)</li> <li>reachable set:   \\(\\mathcal{C}=\\) set of initial points \\(x^0\\)* for which there exists a control such that \\(x(t) = 0\\) for some finite time \\(t\\); </li> </ul> <p>\\(\\(\\mathcal{C}=\\bigcup_{t\\ge 0}\\mathcal{C}(t)\\)\\)</p> <p>Let \\(\\mathbb{M}^{n\\times m}\\) denote the set of all \\(n\\times m\\) matrices. Assume that this and next chapter, the ODE is linear in both the state \\(\\boldsymbol{x}(\\cdot)\\) and the control \\(\\boldsymbol{\\alpha}(\\cdot)\\), and he ODE hasthe form</p> \\[\\left\\{\\begin{array}{l}\\dot{\\boldsymbol{x}}(t)=M \\boldsymbol{x}(t)+N \\boldsymbol{\\alpha}(t) \\\\ \\boldsymbol{x}(0)=x^0\\end{array}\\right.\\] <p>where \\(M\\in \\mathbb{M}^{n\\times n}\\) and \\(N\\in \\mathbb{M}^{n\\times m}\\). Assume the set \\(A\\) of conrol parameters is a cube in \\(\\mathbb{R}^m\\):</p> \\[A=[-1,1]^m=\\{a\\in\\mathbb{R}^m| |a_i|\\le 1, i=1,\\ldots,m\\}\\]"},{"location":"control/optimal_control/optimal_control2/#22-quick-review-of-linear-ode","title":"2.2 Quick review of linear ODE","text":"<p>Definition: let \\(\\boldsymbol{X}(\\cdot):\\mathbb{R}\\to \\mathbb{M}^{n\\times n}\\) be the unique solution of the matrix ODE:</p> \\[\\begin{cases}\\dot{\\boldsymbol{X}}(t)=M \\boldsymbol{X}(t) \\quad (t\\in\\mathbb{R})\\\\ \\boldsymbol{X}(0)=\\boldsymbol{I}.\\end{cases}\\] <p>We call \\(\\boldsymbol{X}(t)\\) the fundamental solution and sometimes write</p> \\[\\boldsymbol{X}(t)=e^{tM}:=\\sum_{t=0}^\\infty \\frac{t^kM^k}{k!}\\] <p>Last formula being the definition of the exponential \\(e^{tM}\\) and observe that</p> \\[\\boldsymbol{X}^{-1}(t)=\\boldsymbol{X}(-t)\\] <p>Theorem 2.1: Solving linear systems of ODE 1. The unique solution of the homogeneous system of ODE 2.    \\(\\(\\begin{cases}\\dot{\\boldsymbol{x}}(t)=M \\boldsymbol{x}(t)\\\\ \\boldsymbol{x}(0)=x^0.\\end{cases}\\)\\)</p> <p>is</p> <p>\\(\\(\\boldsymbol{x}(t)=\\boldsymbol{X}(t)x^0=e^{tM}x^0\\)\\)</p> <ol> <li>The unique solution of the nonhomogeneous system</li> </ol> <p>\\(\\(\\begin{cases}\\dot{\\boldsymbol{x}}(t)=M \\boldsymbol{x}(t) + \\boldsymbol{f}(t)\\\\ \\boldsymbol{x}(0)=x^0.\\end{cases}\\)\\)</p> <p>is</p> <p>\\(\\(\\boldsymbol{x}(t)=\\boldsymbol{X}(t)x^0 + \\boldsymbol{X}(t)\\int_0^t \\boldsymbol{X}^{-1}(s)\\boldsymbol{f}(s)\\mathrm{d}s\\)\\)</p> <p>This expression is the variation of parameters formula.</p>"},{"location":"control/optimal_control/optimal_control2/#23-controllability-of-linear-equations","title":"2.3 Controllability of linear equations","text":"<p>According to the variation of parameters formula, the solution of (linear ODE) for a given control \\(\\boldsymbol{\\alpha}(\u00b7)\\) is</p> \\[\\boldsymbol{x}(t)=\\boldsymbol{X}(t)x^0 + \\boldsymbol{X}(t)\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\] \\[\\begin{aligned}   &amp;x^0\\in\\mathcal{C}(t) \\\\   \\leftrightarrow&amp; \\text{there exists a control } \\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A} \\text{ s.t. } \\boldsymbol{x}(t)=0 \\\\   \\leftrightarrow&amp; 0=\\boldsymbol{X}(t)x^0 + \\boldsymbol{X}(t)\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s \\text{ for some control }\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A} \\\\   \\leftrightarrow&amp; x^0 =-\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s \\text{ for some control }\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A} \\end{aligned}\\] <p>Theorem 2.2: Structure of reachable set</p> <ol> <li>The reachable set \\(\\mathcal{C}\\) is symmetric and convex.</li> <li>Also, if \\(x^0\\in\\mathcal{C}(\\bar{t})\\), then \\(x^0\\in\\mathcal{C}(t)\\) for all times \\(t\\ge \\bar{t}\\)</li> </ol>"},{"location":"control/optimal_control/optimal_control2/#definition_1","title":"Definition","text":"<ol> <li>A set \\(S\\) is symmetric if \\(x\\in S\\) implies \\(-x\\in S\\)</li> <li>The set \\(S\\) is convex if \\(x, \\hat{x}\\in S\\) and \\(0\\le\\lambda\\le 1\\) imply \\(\\lambda x+(1-\\lambda)\\hat{x}\\in S\\)</li> </ol> <p>Proof of theorem 2.2:</p> <ol> <li>(Symmetric) Let \\(t\\ge 0\\) and \\(x^0\\in\\mathcal{C}(t)\\). Then \\(x^0=-\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\) for some admissible control \\(\\boldsymbol{\\alpha}\\in\\mathcal{A}\\).</li> </ol> <p>Therefore \\(-x^0=-\\int_0^t\\boldsymbol{X}^{-1}(s)N(-\\boldsymbol{\\alpha}(s))\\mathrm{d}s\\) and \\(-\\boldsymbol{\\alpha}(s)\\in\\mathcal{A}\\) since set \\(A\\) is symmetric</p> <p>Therefore \\(-x^0\\in\\mathcal{C}(t)\\), and so each set \\(\\mathcal{C}(t)\\) symmetric. It follows that \\(\\mathcal{C}\\) is symmetric</p> <ol> <li>(Convexity) Take \\(x^0, \\hat{x}^0\\in\\mathcal{C}\\) so that \\(x^0\\in\\mathcal{C}, \\hat{x}^0\\in\\mathcal{C}(\\hat{t})\\) for appropriate time \\(t,\\hat{t}\\ge 0\\). Assume \\(t\\le \\hat{t}\\). Then</li> </ol> <p>\\(\\(\\begin{aligned}     x^0 &amp;=-\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s \\quad \\text{ for some control }\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A} \\\\     \\hat{x}^0 &amp;=-\\int_0^{\\hat{t}}\\boldsymbol{X}^{-1}(s)N\\hat{\\boldsymbol{\\alpha}}(s)\\mathrm{d}s \\quad \\text{ for some control }\\hat{\\boldsymbol{\\alpha}}(\\cdot)\\in\\mathcal{A}   \\end{aligned}\\)\\)</p> <p>Define a new control</p> <p>\\(\\(\\tilde{\\boldsymbol{\\alpha}}(s):=\\begin{cases}     \\boldsymbol{\\alpha}(s) &amp; \\text{if } 0\\le s\\le t \\\\     0 &amp; \\text{if } s&gt; t   \\end{cases}\\)\\)</p> <p>Then</p> <p>\\(\\(x^0 =-\\int_0^{\\hat{t}}\\boldsymbol{X}^{-1}(s)N\\tilde{\\boldsymbol{\\alpha}}(s)\\mathrm{d}s\\)\\)   and hence \\(x^0\\in\\mathcal{C}(\\hat{t})\\). Now let \\(0\\le \\lambda\\le 1\\), and observe   \\(\\(\\lambda x^0 + (1-\\lambda)\\hat{x}^0 = -\\int_0^{\\hat{t}}\\boldsymbol{X}^{-1}(s)N(\\lambda\\tilde{\\boldsymbol{\\alpha}}(s) + (1-\\lambda)\\hat{\\boldsymbol{\\alpha}}(s))\\mathrm{d}s\\)\\)   Therefore \\(\\lambda x^0 + (1-\\lambda)\\hat{x}^0\\in\\mathcal{C}(\\hat{t})\\subseteq\\mathcal{C}\\)</p> <ol> <li>Assertion (ii) follows from the foregoing if we take \\(\\bar{t}=\\hat{t}\\).</li> </ol>"},{"location":"control/optimal_control/optimal_control2/#a-simple-example","title":"A simple example","text":"<p>Let \\(n=2\\) and \\(m=1, A=[-1,1]\\), and write \\(\\boldsymbol{x}(t)= \\left(x^1(t), x^2(t)\\right)^T\\). Suppose</p> \\[\\begin{equation*} \\left\\{\\begin{array}{l} \\dot{x}^1=0 \\\\ \\dot{x}^2=\\alpha(t) . \\end{array}\\right. \\end{equation*}\\] <p>This is a system of the form \\(\\dot{\\boldsymbol{x}}=M \\boldsymbol{x}+N \\alpha\\), for</p> \\[\\begin{equation*} M=\\left(\\begin{array}{ll} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{array}\\right), \\quad N=\\binom{0}{1} \\end{equation*}\\] <p>Clearly \\(\\mathcal{C}=\\left\\{\\left(x_1, x_2\\right) \\mid x_1=0\\right\\}\\), the \\(x_2\\)-axis.</p> <p>We next wish to establish some general algebraic conditions ensuring that \\(\\mathcal{C}\\) contains a neighborhood of the origin</p>"},{"location":"control/optimal_control/optimal_control2/#controllability","title":"Controllability","text":"<p>Definition: The controllability matrix is</p> \\[G=G(M,N):=\\underbrace{\\begin{bmatrix}   N &amp; MN &amp; M^2N &amp; \\cdots &amp; M^{n-1}N \\end{bmatrix}}_{n\\times (mn) \\text{ matrix}}\\] <p>Theorem 2.3: Controllability matrix \\(\\(\\operatorname{rank}G=n \\leftrightarrow 0\\in\\mathcal{C}^\\circ\\)\\)</p> <p>Notation:  - \\(\\mathcal{C}^\\circ\\): the interior of the set \\(\\mathcal{C}\\), with its own and neighbor fields in the set - rank of \\(G\\) = number of linearly independent rows / columns of \\(G\\); \\(\\operatorname{rank}G\\le n\\)</p> <p>Proof:</p> <ol> <li>Suppose 1<sup>st</sup> that \\(\\operatorname{rank}G&lt;n\\). This means that the linear span of the columns of G has dimension less than or equal to \\(n-1\\). Thus there exists a vector \\(b \\in \\mathbb{R}^n, b \\neq 0\\), orthogonal to each column of \\(G\\). This implies \\(b^\\top G=0\\). So</li> </ol> <p>\\(\\(b^\\top N = b^\\top MN = \\cdots = b^\\top M^{n-1}N = 0\\)\\)</p> <ol> <li>In fact, \\(b^\\top M^kN=0, \\, \\forall k\\in\\mathbb{R}_+\\)</li> </ol> <p>To confirm this, recall that</p> <p>\\(\\(p(\\lambda):=\\operatorname{det}(\\lambda I-M)\\)\\)</p> <p>is the characteristic polynomial of \\(M\\). The Cayley\u2013Hamilton Theorem states that \\(p(M)=0\\)</p> <p>So if we write</p> <p>\\(\\(p(\\lambda)=\\lambda^n+\\beta_{n-1} \\lambda^{n-1}+\\cdots+\\beta_1 \\lambda^1+\\beta_0\\)\\)</p> <p>then</p> <p>\\(\\(p(M)=M^n+\\beta_{n-1} M^{n-1}+\\cdots+\\beta_1 M+\\beta_0I = 0\\)\\)</p> <p>Therefore</p> <p>\\(\\(M^n=-\\beta_{n-1} M^{n-1}-\\beta_{n-2} M^{n-2}-\\cdots-\\beta_1 M-\\beta_0I\\)\\)</p> <p>and so</p> <p>\\(\\(b^\\top M^nN=b^\\top(-\\beta_{n-1} M^{n-1}-\\cdots)N=0\\)\\)</p> <p>Similarly, \\(b^\\top M^{n+1}N=0\\), etc.</p> <p>Now notice that</p> <p>\\(\\(\\begin{aligned}     b^\\top \\mathbf{X}^{-1}(s) N&amp;=b^\\top e^{-s M} N\\\\     &amp;=b^T \\sum_{k=0}^{\\infty} \\frac{(-s)^k M^k N}{k!}\\\\     &amp;=\\sum_{k=0}^{\\infty} \\frac{(-s)^k}{k!} b^T M^k N=0   \\end{aligned}\\)\\)</p> <ol> <li>Assume next that \\(x^0\\in\\mathcal{C}(t)\\). This is equivalent to having</li> </ol> <p>\\(\\(x^0 =-\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s \\quad \\text{ for some control }\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A}\\)\\)</p> <p>Then</p> <p>\\(\\(b\\cdot x^0 =-\\int_0^tb^\\top\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s =0\\)\\)   This says that \\(b\\) is orthogonal \\(x^0\\). In other words, \\(\\mathcal{C}\\) must lie in the hyperplane orthogonal to \\(b \\neq 0\\). Consequently \\(\\mathcal{C}^\\circ=\\phi\\).</p> <p>(How to understand: in the hyperplane, there is no hypersphere in the set)</p> <ol> <li>Conversely, assume that \\(0 \\notin \\mathcal{C}^\\circ\\). Thus \\(0 \\notin \\mathcal{C}^\\circ(t), \\, \\forall t&gt;0\\). Since \\(\\mathcal{C}(t)\\) is convex, there exits a support hyperplane to \\(\\mathcal{C}(t)\\) through \\(0\\)(This hyperplane put the set into just one side, and \\(0\\) is not in te interior, so can do this). This means that \\(\\exist b\\neq 0\\), s.t. \\(b\\cdot x^0\\le 0, \\forall x^0\\in\\mathcal{C}(t)\\)</li> </ol> <p>(An equation for hyperplane that crosses thre origin is \\(b\\cdot x=0\\))</p> <p>Choose any \\(x^0\\in\\mathcal{C}(t)\\). Then</p> <p>\\(\\(x^0 =-\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\)\\)</p> <p>for some control \\(\\boldsymbol{\\alpha}\\), and therefore</p> <p>\\(\\(0 \\ge b\\cdot x^0 =-\\int_0^tb^\\top\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\)\\)</p> <p>Thus</p> <p>\\(\\(\\int_0^tb^\\top\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\ge 0 \\text{ for all controls } \\boldsymbol{\\alpha}(\\cdot)\\)\\)</p> <p>We assert that therefore</p> <p>\\(\\(b^\\top \\boldsymbol{X}^{-1}(s)N\\equiv 0\\)\\)</p> <p>a proof of which follows as a lemma below. We rewrite it as</p> <p>\\(\\(b^\\top e^{-sM}N\\equiv 0\\)\\)</p> <p>Let \\(s=0\\) to see that \\(b^\\top N=0\\). Next differentiate it with respect to s, to find that</p> <p>\\(\\(b^\\top (-M)e^{-sM}N\\equiv 0\\)\\)</p> <p>For \\(s=0\\) this says</p> <p>\\(\\(b^\\top MN=0\\)\\)</p> <p>We repeatedly differentiate, to deduce</p> <p>\\(\\(b^\\top M^kN=0, \\quad \\forall =0,1,\\cdots\\)\\)</p> <p>and so \\(b^\\top G=0\\). This implies \\(\\operatorname{rank}G&lt;n\\), since \\(b\\neq 0\\).</p> <p>LEMMA 2.4: Integral inequalities</p> <p>Assume that</p> \\[\\int_0^t b^\\top \\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\ge 0$$ for all controls $\\boldsymbol{\\alpha}(\\cdot)$. Then $$b^\\top \\boldsymbol{X}^{-1}(s)N\\equiv 0\\] <p>Proof: Replacing \\(\\boldsymbol{\\alpha}\\) with \\(-\\boldsymbol{\\alpha}\\), we see that</p> \\[\\int_0^t b^\\top \\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s= 0\\] <p>for all controls \\(\\boldsymbol{\\alpha}(\\cdot)\\).</p> <p>Define </p> \\[\\boldsymbol{v}(s):=b^\\top \\boldsymbol{X}^{-1}(s)N\\] <p>If \\(\\boldsymbol{v}\\neq 0\\), then \\(\\boldsymbol{v}(s_0)\\neq 0\\) for some \\(s_0\\). Then there exists an interval \\(I\\) s.t. \\(s_0\\in I\\) and \\(\\boldsymbol{v}(s)\\neq 0\\) on \\(I\\). Now define \\(\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A}\\) this way:</p> \\[\\begin{cases} \\boldsymbol{\\alpha}(s)=0, \\quad (s\\notin I)\\\\ \\boldsymbol{\\alpha}(s)=\\frac{\\boldsymbol{v}(s)}{|\\boldsymbol{v}(s)|}\\frac{1}{\\sqrt{n}}, \\quad (s\\in I) \\end{cases}\\] <p>Then</p> \\[0=\\int_0^t \\boldsymbol{v}(s)\\boldsymbol{\\alpha}(s)\\mathrm{d}s = \\int_I \\frac{\\boldsymbol{v}(s)}{\\sqrt{n}}\\frac{\\boldsymbol{v}(s)}{|\\boldsymbol{v}(s)|}\\mathrm{d}s = \\frac{1}{\\sqrt{n}}\\int_I |\\boldsymbol{v}(s)|\\mathrm{d}s\\] <p>This implies the contradiction that \\(\\boldsymbol{v}\\equiv 0\\) in \\(I\\).</p> <p>Definition: We say the linear system (ODE) is controllable if \\(\\mathcal{C}=\\mathbb{R}^n\\)</p> <p>Theorem 2.5: Criterion for controllability Let \\(A\\) be the cube \\([-1,1]^n\\) in \\(\\mathbb{R}^n\\). Suppose as well that \\(\\operatorname{rank}G=n\\), and \\(\\operatorname{Re}\\lambda &lt; 0\\) for each eigenvalue \\(\\lambda\\) of the matrix \\(M\\). Then the system(ODE) is controllable</p> <p>Proof: Since \\(\\operatorname{rank}G=n\\), Theroem 2.3 tells us that \\(\\mathcal{C}\\) contains some ball \\(B\\) centered at \\(0\\). Now take any \\(x^0\\in\\mathbb{R}^n\\) and consider the evolution</p> \\[ \\begin{cases} \\dot{\\boldsymbol{x}}(t)=M\\boldsymbol{x}(t) \\\\ \\boldsymbol{x}(0)=x^0 \\end{cases} \\] <p>in other words, take the control \\(\\boldsymbol{\\alpha}(\\cdot)\\equiv 0\\). Since \\(\\operatorname{Re}\\lambda &lt; 0\\) for each eigenvalue \\(\\lambda\\) of the matrix \\(M\\), then the origin is asymptotically stable. So ther exists a time \\(T\\) s.t. \\(\\boldsymbol{x}(t)\\in B\\). Thus \\(\\boldsymbol{x}(T)\\in B\\subset \\mathcal{C}\\); and hence there exists a control \\(\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A}\\) steering \\(\\boldsymbol{x}(t)\\) into \\(0\\) in finite time.</p> <p>Example We once again consider the rocket railroad car, from \u00a71.2, for which \\(n=2,m=1,A=[-1,1]\\), and</p> \\[\\dot{\\boldsymbol{x}}= \\begin{bmatrix}   0 &amp; 1 \\\\   0 &amp; 0 \\\\ \\end{bmatrix} \\boldsymbol{x} +  \\begin{bmatrix}   0 \\\\   1 \\end{bmatrix}\\alpha $$ Then $$G=[N, MN] =  \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix} \\] <p>Therefore \\(\\operatorname{rank}G=2=n\\)</p> <p>Also, the characteristic polynomial of the matrix \\(M\\) is</p> \\[p(\\lambda)=\\operatorname{det}(\\lambda I-M)=\\operatorname{det} \\begin{pmatrix}   \\lambda &amp; -1 \\\\   0 &amp; \\lambda \\end{pmatrix} = \\lambda^2\\] <p>Since the eigenvalues are both \\(0\\), we fail to satisfy the hypotheses of Theorem 2.5. </p> <p>This example motivates the following extension of the previous theorem:</p> <p>Theorem 2.6.: Improved criterion for controllability Assume \\(\\operatorname{rank}G=n\\) and \\(\\operatorname{Re}\\lambda \\le 0\\) for each eigenvalue \\(\\lambda\\) of \\(M\\). Then the system(ODE) is controllable.</p> <p>Proof:</p> <ol> <li>If \\(\\mathcal{C}\\neq \\mathbb{R}^n\\), then the convexity of \\(\\mathcal{C}\\) implies that there exists a vector \\(b\\neq 0\\) and a real number \\(\\mu\\) s.t.</li> </ol> <p>\\(\\(b\\cdot x^0\\le\\mu, \\quad \\forall x^0\\in\\mathcal{C}\\)\\)</p> <p>(Must contain a support hyperplane if \\(\\mathcal{C}\\) doesn't contain the whole space)</p> <p>Indeed, in the picture we see that \\(b\\cdot(x^0-z^0)\\le 0\\); and this implies that \\(\\mu:=b\\cdot z^0\\).</p> <p></p> <p>We will derive a contradiction.</p> <ol> <li>Given \\(b\\neq 0, \\mu\\in\\mathbb{R}\\), our intention is to find \\(x^0\\in \\mathcal{C}\\) s.t. \\(b\\cdot x^0\\le\\mu\\) fails. Recall \\(x^0\\in \\mathcal{C}\\) iff \\(\\exist t&gt;0\\) and a control \\(\\boldsymbol{\\alpha}(\\cdot)\\in\\mathcal{A}\\) s.t. </li> </ol> <p>\\(\\(x^0=-\\int_0^t\\boldsymbol{X}^{-1}(s)N\\boldsymbol{\\alpha}(s)\\mathrm{d}s\\)\\)</p> <p>Then</p> <p>$$   bcdot x<sup>0=-int_0</sup>tb<sup>topboldsymbol{X}</sup>s   $$}(s)Nboldsymbol{alpha}(s)mathrm{d</p> <p>Define</p> <p>\\(\\(\\boldsymbol{v}(s):=b^\\top\\boldsymbol{X}^{-1}(s)N\\)\\)</p> <ol> <li>We assert that \\(\\boldsymbol{v}\\neq 0\\)</li> </ol> <p>To see this, suppose instead that \\(\\boldsymbol{v}\\equiv 0\\). Then \\(k\\) times differentiate the expression \\(b^\\top \\boldsymbol{X}^{-1}(s)N\\) w.r.t. \\(s\\) and set \\(s=0\\), to discover</p> <p>\\(\\(b^\\top M^kN=0, \\quad k=0,1,2,\\ldots\\)\\)</p> <p>This implies \\(b\\) is orthogonal to the columns of \\(G\\), and so \\(\\operatorname{rank}G&lt;n\\). This is a contradiction to our hypothesis, and therefore \\(\\boldsymbol{v}\\neq 0\\) holds.</p> <ol> <li>Next, define \\(\\boldsymbol{\\alpha}(\\cdot)\\) this ay:</li> </ol> <p>$$boldsymbol{alpha}(s):=   begin{cases}     -frac{boldsymbol{v}(s)}{|boldsymbol{v}(s)|}, &amp; text{if } boldsymbol{v}(s)neq 0, \\     0, &amp; text{if } boldsymbol{v}(s)=0.   end{cases}   $$</p> <p>Then</p> <p>$$bcdot x^0 = -int_0^t boldsymbol{v}(s)boldsymbol{alpha}(s)mathrm{d}s = int_0^t |boldsymbol{v}(s)|mathrm{d}s   $$</p> <p>We want to find a time \\(t&gt;0\\) s.t. \\(\\int_0^t |\\boldsymbol{v}(s)|\\mathrm{d}s&gt;\\mu\\). In fact, we assert that</p> \\[\\int_0^\\infty |\\boldsymbol{v}(s)|\\mathrm{d}s=+\\infty\\] <p>To begin the proof above introduce the function  </p> <p>$$   phi(t):=int_t^inftyboldsymbol{v}(s)mathrm{d}s   $$</p> <p>We will find an ODE \\(\\phi\\) satisfies. Take \\(p(\\cdot)\\) to be the characteristic polynomial of \\(M\\). Then</p> <p>$$   begin{aligned}     &amp;pleft(-frac{mathrm{d}}{mathrm{d}t}right)boldsymbol{v}(t)\\     =&amp;pleft(-frac{mathrm{d}}{mathrm{d}t}right)[b^top e^{-tM}N]\\     =&amp;b^top left(pleft(-frac{mathrm{d}}{mathrm{d}t}right)e^{-tM}right)N \\     =&amp;b^top left(pleft(Mright)e^{-tM}right)N equiv 0   end{aligned}   $$</p> <p>Since \\(p(M)=0\\), according to the Cayley\u2013Hamilton Theorem. But since \\(p\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right)\\boldsymbol{v}(t)\\equiv 0\\), it follows that </p> <p>\\(\\(-\\frac{\\mathrm{d}}{\\mathrm{d}t}p\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right)\\phi(t) = p\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right)\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\phi\\right)=p\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right)\\boldsymbol{v}(t)=0\\)\\)</p> <p>Hence \\(\\phi\\) solves the (n+1)th order ODE</p> <p>\\(\\(\\frac{\\mathrm{d}}{\\mathrm{d}t}p\\left(-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\right)\\phi(t) = 0\\)\\)</p> <p>We also know that \\(\\phi(\\cdot)\\neq 0\\). Let \\(\\mu_1,\\cdots,\\mu_{n+1}\\) be the solutions of \\(\\mu p(-\\mu)=0\\). According to ODE theory, we can write</p> <p>\\(\\phi(t)=\\) sum of the terms of the form \\(p_i(t)e^{\\mu_i t}\\)</p> <p>for appropriate polynomials \\(p_i(\\cdot)\\)</p> <p>Furthermore, we see that \\(\\mu_{n+1}=0\\) and \\(\\mu_k=-\\lambda_k\\), where \\(\\lambda_1,\\cdots,\\lambda_n\\) are the eigenvalues of \\(M\\). By assumption \\(\\operatorname{Re}\\mu_k\\ge 0, \\quad k=0,1,\\cdots,n\\). If \\(\\int_0^\\infty |\\boldsymbol{v}(s)|\\mathrm{d}s&lt;\\infty\\), then</p> <p>\\(\\(|\\phi(t)|\\le\\int_0^\\infty |\\boldsymbol{v}(s)|\\mathrm{d}s\\to 0, \\quad \\text{as }t\\to\\infty\\)\\)</p> <p>that is, \\(\\phi(t)\\to 0\\) as \\(t\\to\\infty\\). This's a contradiction to the representation formula of \\(\\phi(t)=\\sum p_i(t)e^{\\mu_i t}\\), with \\(\\operatorname{Re}\\mu_i\\ge 0\\). Assertaion is proved.</p> <ol> <li>Consequently given any \\(\\mu, \\exist t&gt;0\\) s.t.</li> </ol> <p>\\(\\(b\\cdot x^0=\\int_0^t |\\boldsymbol{v}(s)|\\mathrm{d}s &gt; \\mu\\)\\)</p> <p>a contradiction to (2.8). Therefore \\(\\mathcal{C}=\\mathbb{R}^n\\).</p>"},{"location":"control/optimal_control/optimal_control2/#24-observability","title":"2.4 Observability","text":"<p>Consider the linear system of ODE</p> \\[\\begin{cases}\\dot{\\boldsymbol{x}}(t)=M \\boldsymbol{x}(t)\\\\ \\boldsymbol{x}(0)=x^0.\\end{cases}\\] <p>where \\(M\\in\\mathbb{M}^{n\\times n}\\).</p> <p>In this section we address the observability problem, modeled as follows. We suppose that we can observe</p> \\[\\boldsymbol{y}(t):=N\\boldsymbol{x}(t)\\quad (t\\ge 0)\\] <p>for a given matrix \\(N\\in\\mathbb{M}^{m\\times n}\\). Consequently, \\(\\boldsymbol{y}(t)\\in\\mathbb{R}^{m}\\). The interesting situation is when \\(m&lt;&lt;n\\) and we interpret \\(\\boldsymbol{y}(\\cdot)\\) as low-dimensional \u201cobservations\u201d or \u201cmeasurements\u201d of the high-dimensional dynamics \\(\\boldsymbol{x}(\\cdot)\\)</p> <p>Observability question: Given the observation \\(\\boldsymbol{y}(\\cdot)\\), can we in principle reconstruct \\(\\boldsymbol{x}(\\cdot)\\)? In particular, do observations of \\(\\boldsymbol{y}(\\cdot)\\) provide enough information for us to deduce the initial value \\(x^0\\) for (ODE)?</p> <p>Definition: The pair (ODE, Observation) called observable if the knowledge of \\(\\boldsymbol{y}(\\cdot)\\) on any time interval \\([0, t]\\) allows us to compute \\(x^0\\).</p> <p>More precisely, (ODE, Observation) is observable if for all solutions \\(\\boldsymbol{x}_1(\\cdot), \\boldsymbol{x}_2(\\cdot), N\\boldsymbol{x}_1(\\cdot)\\equiv N\\boldsymbol{x}_2(\\cdot)\\) on a time interval \\([0, t]\\) implies \\(\\boldsymbol{x}_1(0)=\\boldsymbol{x}_2(0)\\).</p> <p>2 simple examples 1. If \\(N\\equiv 0\\), then clearly the system is not observable. 2. On the other hand, if \\(m = n\\) and \\(N\\) is invertible, then clearly \\(\\boldsymbol{x}(t) = N^{\u22121}\\boldsymbol{y}(t)\\) is observable.</p> <p>The interesting cases lie between these extremes.</p> <p>Theorem 2.7: Observability and controllability The system 1</p> \\[ \\begin{cases}   \\dot{\\boldsymbol{x}}(t) = M\\boldsymbol{x}(t) \\\\   \\boldsymbol{y}(t) = N\\boldsymbol{x}(t) \\end{cases} \\] <p>is observable iff the system 2</p> \\[ \\dot{\\boldsymbol{z}}(t) = M^\\top \\boldsymbol{z}(t) + N^\\top \\boldsymbol{\\alpha}(t), \\quad A=\\mathbb{R}^m \\] <p>is controllable, meaning that \\(\\mathcal{C}=\\mathbb{R}^n\\)</p> <p>INTERPRETATION. This theorem asserts that somehow \u201cobservability and controllability are dual concepts\u201d for linear systems.</p> <p>Proof:</p> <ol> <li>(\\(\\leftarrow\\)) Suppose the system 1 is not observable. Then \\(\\exist x^1\\neq x^2\\in\\mathbb{R}^n\\), s.t.</li> </ol> <p>$$    begin{cases}      dot{boldsymbol{x}}_1(t) = Mboldsymbol{x}_1(t), quad boldsymbol{x}_1(0)=x^1 \\      dot{boldsymbol{x}}_2(t) = Mboldsymbol{x}_2(t), quad boldsymbol{x}_2(0)=x^2    end{cases}    $$</p> <p>but \\(\\boldsymbol{y}(t):=N\\boldsymbol{x}_1(t)\\equiv N\\boldsymbol{x}_2(t), \\forall t\\ge 0\\). Let</p> <p>\\(\\(\\boldsymbol{x}(t):=\\boldsymbol{x}_1(t) - \\boldsymbol{x}_2(t), \\quad x^0:=x^1-x^2\\)\\)</p> <p>Then</p> <p>\\(\\(\\dot{\\boldsymbol{x}}(t)=M\\boldsymbol{x}(t), \\quad\\boldsymbol{x}(0)=x^0\\neq 0\\)\\)</p> <p>but</p> <p>\\(\\(N\\boldsymbol{x}(t)=0 \\quad (t\\ge 0)\\)\\)</p> <p>Now</p> <p>\\(\\(\\boldsymbol{x}(t)=\\boldsymbol{X}(t)x^0 = e^{tM}x^0\\)\\)</p> <p>Thus</p> <p>\\(\\(Ne^{tM}x^0=0 \\quad (t\\ge 0)\\)\\)</p> <p>Let \\(t=0\\), to find \\(Nx^0=0\\). Then differentiate this expression \\(k\\) times in \\(t\\) and let \\(t = 0\\), to discover as well that</p> <p>\\(\\(NM^kx^0=0\\)\\)</p> <p>for \\(k=0,1,2\\cdots\\) Hence \\((x^0)^\\top(M^k)^\\top N^\\top = 0\\) and hence \\((x^0)^\\top(M^\\top)^kN^\\top = 0\\). This implies</p> \\[(x^0)^\\top[N^\\top, M^\\top N^\\top, \\cdots, (M^\\top)^{n-1}N^\\top]=0\\] <p>Since \\(x^0\\neq 0, \\operatorname{rank}[N^\\top, \\cdots, (M^\\top)^{n-1}N^\\top]&lt;n\\). Thus system 2 is not controllable. Consequently, system 2 controllable implies system 1 is observable.</p> <ol> <li>(\\(\\to\\))Assume now system 2 is not controllable. Then \\(\\operatorname{rank}[N^\\top, \\cdots,(M^\\top)^{n-1}N^\\top]&lt;n\\), and consequently according to Theorem 2.3, \\(\\exist x^0\\neq 0\\), s.t.</li> </ol> <p>\\(\\((x^0)^\\top[N^\\top, \\cdots,(M^\\top)^{n-1}N^\\top]=0\\)\\)</p> <p>That is, \\(NM^kx^0=0, \\, \\forall k=0,1,2,\\cdots,n-1\\)</p> <p>We want to show that \\(\\boldsymbol{y}(t)=N\\boldsymbol{x}(t)\\equiv 0\\), where </p> \\[\\begin{cases}\\dot{\\boldsymbol{x}}(t)=M \\boldsymbol{x}(t)\\\\ \\boldsymbol{x}(0)=x^0.\\end{cases}\\] <p>According to the Cayley\u2013Hamilton Theorem, we can write</p> <p>\\(\\(M^n = -\\beta_{n-1}M^{n-1} - \\cdots - \\beta_0I\\)\\)</p> <p>for appropriate constants. Consequently \\(NM^nx^0=0\\). Likewise</p> <p>$$   begin{aligned}   M<sup>{n+1}&amp;=M(-beta_{n-1}M</sup> - cdots - beta_0I) \\   &amp;= -beta_{n-1}M^{n} - cdots - beta_0M   end{aligned}   $$</p> <p>and so \\(NM^{n+1}x^0=0\\). Similarly, \\(NM^kx^0=0, \\, \\forall k\\).</p> <p>Now</p> \\[\\boldsymbol{x}(t)=\\boldsymbol{X}(t)x^0 = e^{Mt}x^0 = \\sum_{k=0}^\\infty \\frac{t^kM^k}{k!}x^0\\] <p>and therefore \\(N\\boldsymbol{x}(t)=N\\sum\\limits_{k=0}^\\infty \\frac{t^kM^k}{k!}x^0 = 0\\)</p> <p>We have shown that if system 2 is not controllable, then system 1 is not observable.</p>"},{"location":"math/eng_method/linear_algebra1/","title":"Introduction of Linear algebra","text":"<p> \u7ea6 3508 \u4e2a\u5b57  \u9884\u8ba1\u9605\u8bfb\u65f6\u95f4 14 \u5206\u949f</p>"},{"location":"math/eng_method/linear_algebra1/#sets","title":"sets","text":"<p>Definition</p> <p>sets: collection of objects, called elements of the set - \\(x\\in A; x\\notin A\\) - list them using \\(\\{\\}\\) - no order: \\(\\{1,2\\}=\\{2,1\\}\\)</p> <p>subsets: </p> <ul> <li>\\(B\\subseteq A\\)</li> <li>proper set: \\(B\\subseteq A, B\\neq A\\)</li> <li>empty set: \\(\\phi\\)</li> </ul> <p>calculation</p> <ul> <li>union: \\(A\\cup B\\)</li> <li>intersection: \\(A \\cap B\\)</li> <li> <p>disjoint sets: \\(A\\cap B=\\phi\\)</p> </li> <li> <p>finite sets: \\(\\{1,2\\}\\)</p> </li> <li>infinite sets: \\(\\mathbb{R}\\)</li> </ul> <p>Number systems:</p> <ul> <li>\\(\\mathbb{N}(\\mathbb{N}_0: (\\text{ include }0); \\mathbb{N}_1: (\\text{ not include }0))\\)</li> <li>\\(\\mathbb{Z}\\)</li> <li>Rational number: \\(\\mathbb{Q}\\)</li> <li>Real number: \\(\\mathbb{R}\\)</li> <li>Complex number: \\(\\mathbb{C}\\)</li> </ul> <p>Cardinal number: the number if elements of a set</p> <p>For infinite sets:</p> <ul> <li>Countable sets: aleph zero: \\(\\mathbb{N},\\mathbb{Z},\\mathbb{Q}\\)</li> <li>Uncountable sets: beth one</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#operations","title":"operations","text":"<p>operation: from a set to itself</p> <p>Unary:</p> <ul> <li>negation: \\(-\\)</li> <li>factorial: \\(!\\)</li> <li>trigonometric: \\(\\sin,\\cos\\)</li> </ul> <p>Binary:</p> <ul> <li>\\(+-x/\\)</li> </ul> <p>tenary:\u2026</p>"},{"location":"math/eng_method/linear_algebra1/#arithmetic","title":"Arithmetic","text":"<ul> <li>\\(+,\\times\\)</li> </ul> Set \\(\\mathbb{N}_0\\) \\(\\mathbb{N}_1\\) Elements \\(0,1,2,3\\in\\mathbb{N}_0\\) \\(1,2,3,4\\in\\mathbb{N}_1\\) Operation \\(+\\) \\(\\times\\) Closure \\(1+2=3\\in\\mathbb{N}_0\\) \\(2\\times 3=6\\in\\mathbb{N}_1\\) Associative \\((1+2)+3=1+(2+3)\\) \\((2\\times 3)\\times 4=2\\times(3\\times 4)\\) Commutative \\(1+2=2+1\\) \\(2\\times 3=3\\times 2\\) Distributive - - - \\(-,/\\) Set \\(\\mathbb{Z}\\) \\(\\mathbb{Q}\\) Elements \\(x,y,\\cdots\\in\\mathbb{Z}\\) \\(x,y\\cdots\\in\\mathbb{Q}\\) Operation \\(+\\) \\(x\\) Identity \\(0\\) \\(1\\) inverses \\(-x\\) \\(\\frac{1}{x}\\) or \\(x^{-1}\\) <p>More generally:</p> Set $ S$ \\(\\mathbb{Z}\\) \\(\\mathbb{Q}\\) Elements \\(x,y,z,\\cdots\\in S\\) \\(x,y,z,\\cdots\\in\\mathbb{Z}\\) \\(x,y,z,\\cdots\\in\\mathbb{Q}\\) Operation \\(*\\) \\(+\\) \\(\\times\\) Closure \\(x*y\\in S\\) \\(x+y\\in\\mathbb{Z}\\) \\(x\\times y\\in\\mathbb{Q}\\) Associative \\((x*y)*z=x*(y*z)\\) \\((x+y)+z=x+(y+z)\\) \\((x\\times y)\\times z=x\\times(y\\times z)\\) Commutative \\(x*y=y*x\\) \\(x+y=y+x\\) \\(x\\times y=y\\times x\\) Distributive - - - Identity \\(\\exists e\\in S: x*e=e*x=x\\) \\(0\\) \\(1\\) inverses \\(\\forall x,\\exists x^{-1}\\in S: x*x^{-1}=x^{-1}*x=e\\) \\(-x\\) \\(\\frac{1}{x}\\) or \\(x^{-1}\\)"},{"location":"math/eng_method/linear_algebra1/#group","title":"group","text":"<p>Given:</p> <ul> <li>Set of elements \\(G\\)</li> <li>Operation: \\(*\\)</li> </ul> Set \\(G\\) Elements \\(x,y,z,\\cdots\\in G\\) Operation \\(*\\) Closure \\(x*y\\in G\\) Associative \\((x*y)*z=x*(y*z)\\) Commutative \\(x*y=y*x\\)? (Not requestes) Distributive - Identity \\(\\exists e\\in G: x*e=e*x=x\\) inverses \\(\\forall x,\\exists x^{-1}\\in G: x*x^{-1}=x^{-1}*x=e\\) <ul> <li>If communtative: commutative group / abelian group</li> <li>If not commutative: non-commutative group / non-abelian group</li> </ul> <p>Example of group:</p> <ul> <li>commutative:</li> <li>\\(\\mathbb{Z}\\) with \\(+\\)</li> <li>\\(\\mathbb{Z}\\) commutes under \\(+\\)</li> <li>non-commutative:</li> <li>Dihedral group TBC</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#ring","title":"Ring","text":"<p>Given:</p> <ul> <li>Set of elements \\(R\\)</li> <li>Operation: \\(+, \\times\\)</li> </ul> Set \\(R\\) Elements \\(a,b,c,\\cdots\\in R\\) Operation Addition \\(+\\) Multiplication \\(\\times\\) Closure \\(a+b\\in R\\) \\(a\\times b\\in R\\) Associative \\((a+b)+c=a+(b+c)\\) \\((a\\times b)\\times c=a\\times(b\\times c)\\) Commutative \\(a+b=b+a\\) \\(a\\times b=b\\times a\\)? (Not required) Distributive \\(a\\times(b+c)=a\\times b + a\\times c; \\\\ (b+c)\\times a=b\\times a+c\\times a\\) Identity \\(\\exists 0\\in R: a+0=0+a=a\\) \\(\\exists 1\\in R: 1\\times a = a\\times 1 = a\\) Inverses \\(\\exists (-a)\\in R: a+(-a)=0\\) \\(\\exists a^{-1}\\in R\\)? (Not required) <ul> <li>distributive: left distributive v.s. right distributive</li> <li>If communitative, just check one distributivity</li> </ul> <p>Example of ring:</p> <ul> <li>commutative:</li> <li>\\(\\mathbb{Z}\\) with \\(+,\\times\\)</li> <li>non-commutative:</li> <li>\\(M_2(R)\\) with matrix addition and matrix multiplication</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#field","title":"Field","text":"<p>Given:</p> <ul> <li>Set of elements \\(F\\)</li> <li>Operation: \\(+, \\times\\)</li> </ul> Set \\(F\\) Elements \\(a,b,c,\\cdots\\in F\\) Operation Addition \\(+\\) Closure \\(a+b\\in F\\) Associative \\((a+b)+c=a+(b+c)\\) Commutative \\(a+b=b+a\\) Distributive \\(a\\times(b+c)=a\\times b + a\\times c\\) Identity \\(\\exists 0\\in F: a+0=0+a=a\\) inverses \\(\\exists (-a)\\in F: a+(-a)=0\\) <p>Example of field:</p> <ul> <li>\\(\\mathbb{Q}\\) with \\(+,\\times\\)</li> <li>\\(\\mathbb{R}\\) with \\(+,\\times\\)</li> <li>\\(\\mathbb{C}\\) with \\(+,\\times\\)</li> <li>\\(\\operatorname{GF}(2) / Z_2\\), a finite field with two elements with XOR and AND.</li> </ul> <p>Modulo Arithmetic:</p> <ul> <li>\\(17 \\pmod 5 = 2\\)</li> <li>\\(17 \\equiv 2 (\\pmod 5)\\)</li> </ul> <p>Theorem C.1 (Cancellation Laws):</p> <p>\\(\\forall a,b,c\\) in a field, the following statements are true:</p> <ul> <li>If \\(a+b=c+b\\), then \\(a=c\\)</li> <li>If \\(a\\cdot b=c\\cdot b,b\\neq 0\\), then \\(a=c\\)</li> </ul> <p>Corollary: the identity elements and the inverse elements are unique</p> <p>Theorem C.2:</p> <p>\\(\\forall a,b\\) in a field, the following statements are true:</p> <ul> <li>\\(a\\cdot 0=0\\)</li> <li>\\((-a)\\cdot b=a\\cdot(-b)=-(a\\cdot b)\\)</li> <li>\\((-a)\\cdot(-b)=a\\cdot b\\)</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#vector-space","title":"vector space","text":"<p>Given:</p> <ul> <li>Set of elements \\(V, F\\)</li> <li>Operation: \\(+, \\times\\)</li> <li>Commutative group \\(V\\) under \\(+\\), with a Field \\(F\\)</li> </ul> Set \\(V\\) \\(V\\) and \\(F\\) \\(F\\) \\(F\\) Elements \\(\\vec{u},\\vec{v},\\vec{w}\\in V\\) \\(a,b,c\\in F\\) Operation Addition \\(+\\) Scalar multiplication \\(\\times\\) Addition \\(+\\) Multiplication \\(\\times\\) Closure \\(\\vec{u}+\\vec{v}\\in V\\) \\(a\\times \\vec{u}\\in V\\) \\(a+b\\in F\\) \\(a\\times b\\in F\\) Associative \\((\\vec{u}+\\vec{v})+\\vec{w} = \\vec{u}+(\\vec{v}+\\vec{w})\\) \\((a\\times b)\\times \\vec{u}=a\\times(b\\times \\vec{u})\\) \\((a+b)+c=a+(b+c)\\) \\((a\\times b)\\times c=a\\times(b\\times c)\\) Commutative \\(\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}\\) color{gray}{\\(a\\times \\vec{u}=\\vec{u}\\times a\\)} \\(a+b=b+a\\) \\(a\\times b=b\\times a\\) Distributive - \\(a\\times(\\vec{u}+\\vec{v})=a\\times\\vec{u}+a\\times\\vec{v}; (a+b)\\times\\vec{u}=a\\times\\vec{u}+b\\times\\vec{u}\\) \\(a\\times(b+c)=a\\times b + a\\times c\\) Identity \\(\\exists \\vec{0}\\in V: \\vec{u}+\\vec{0}=\\vec{0}+\\vec{u}=\\vec{u}\\) \\(1\\times \\vec{u}=\\vec{u}\\) \\(\\exists 0\\in F: a+0=0+a=a\\) \\(\\exists 1\\in F: 1\\times a = a\\times 1 = a\\) inverses \\(\\exists (-\\vec{u})\\in V: \\vec{u}+(-\\vec{u})=\\vec{0}\\) color{gray}{\\(0\\times u=\\vec{0}; (-1)\\times \\vec{u}=-\\vec{u}\\)} \\(\\exists (-a)\\in F: a+(-a)=0\\) \\(\\exists a^{-1}\\in F: a\\times a^{-1}=1\\) <p>Module definition: similar to vector space, but:</p> <ul> <li>commutative of scalar multiplication not required</li> <li>commutative of multiplication for number part, not required</li> </ul> <p>Definition:</p> <ul> <li>sum: \\(x+y\\)</li> <li>product: \\(ax\\)</li> <li>scalars: elements of \\(F\\)</li> <li>vectors: elements of vector space \\(\\mathsf{V}\\)</li> <li>n-tuple: \\(n\\) elements of a field in this form: \\((a_1,\\cdots,a_n)\\)</li> <li>entries / components: \\(a_1,\\cdots,a_n\\)</li> <li>2 n-tuples are equal if \\(a_i=b_i, \\forall i=01,2,\\cdots,n\\)</li> <li>\\(\\mathsf{F}^n\\): set of all n-tuples with entries from a field \\(F\\) </li> <li>vectors in \\(\\mathsf{F}^n\\): column vectors</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#addition-and-scalar-multiplication","title":"addition and scalar multiplication","text":""},{"location":"math/eng_method/linear_algebra1/#matrix","title":"matrix","text":"<p>Definitions:</p> <ul> <li>diagonal entries: \\(a_{ij}\\) with \\(i=j\\)</li> <li>i-th row: \\(a_{i1},a_{i2},\\cdots,a_{in}\\)</li> <li>j-th column: \\(a_{1j},a_{2j},\\cdots,a_{mj}\\)</li> <li>zero matrix: all zero</li> <li>square: the number of rows and columns are equal</li> <li>equal: \\(A_{ij}=B_{ij}, \\forall 1\\le i\\le m, 1\\le j\\le n\\)</li> <li>set of all \\(m\\times n\\) matrices with entries from a field \\(F\\) is a vector space: \\(\\mathsf{M}_{m\\times n}(F)\\)</li> </ul> <p>matrix addition: \\((A+B)_{ij}=A_{ij}+B_{ij}\\) scalar multiplication: \\((cA)_{ij}=cA_{ij}\\)</p>"},{"location":"math/eng_method/linear_algebra1/#function","title":"function","text":"<p>Let \\(S\\) be any nonempty set and \\(F\\) be any field, and let \\(\\mathcal{F}(S, F)\\) denote the set of all functions from \\(S\\) to \\(F\\). Two functions \\(f\\) and \\(g\\) in \\(\\mathcal{F}(S, F)\\) are called equal if \\(f(s)=g(s)\\) for each \\(s\\in S\\). The set \\(\\mathcal{F}(S, F)\\) is a vector space with the operations of addition and scalar multiplication defined for \\(f, g \\in \\mathcal{F}(S, F)\\) and \\(c\\in F\\) by</p> \\[(f+g)(s)=f(s)+g(s) \\text{ and } (cf)(s)=c[f(s)]\\] <p>for each \\(s \\in S\\). Note that these are the familiar operations of addition and scalar multiplication for functions used in algebra and calculus.</p>"},{"location":"math/eng_method/linear_algebra1/#polynominal","title":"polynominal","text":"\\[f(x)=a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x_1+a_0\\] <ul> <li>coefficient: \\(a_i, i=0,1,\\cdots,n\\)</li> <li>zero polynominal: \\(a_i=0, i=0,1,\\cdots,n\\)</li> <li>degree: </li> <li>\\(-1\\) for zero polynominal</li> <li>largest exponent of \\(x\\)</li> <li>equal if equal degree and \\(a_i=b_i, i=0,1,\\cdots,n\\)</li> </ul> <p>When \\(F\\) is a field containing infinitely many scalars, we usually regard a polynomial with coefficients from \\(F\\) as a function from \\(F\\) into \\(F\\)</p> \\[ \\begin{aligned}   f(x) &amp;= a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x_1+a_0 \\\\   g(x) &amp;= b_nx^n+b_{n-1}x^{n-1}+\\cdots+b_1x_1+b_0 \\end{aligned} \\] <p>addition and scalar multiplication: $$ begin{cases}   f(x)+g(x)=(a_n+b_n)x^n + (a_{n-1}+b_{n-1})x^{n-1}+cdots+(a_0+b_0)\\   cf(x)= ca_nx<sup>n+ca_{n-1}x</sup>+cdots+ca_1x_1+ca_0 end{cases} $$</p> <p>set of all polynominal: \\(\\mathsf{P}(F)\\)</p> <p>Theorem 1.1: Cancellation Law for Vector Addition</p> <p>If \\(x,y,z\\) are vectors in a vector space, s.t. \\(x+z=y+z\\), then \\(x=y\\)</p> <p>Corolloary 1: The vector \\(\\mathbf{0}\\) is unique (zero vector)</p> <p>Corolloary 2: The inverse element of vector is unique (additive inverse)</p> <p>Theorem 1.2: In any vector space \\(\\mathsf{V}\\):</p> <ul> <li>\\(0\\boldsymbol{x}=\\mathbf{0}, \\forall \\boldsymbol{x}\\in\\mathsf{V}\\)</li> <li>\\((-a)x=-(ax)=a(-x), \\forall a\\in F, x\\in \\mathsf{V}\\)</li> <li>\\(a\\mathbf{0}=\\mathbf{0}, \\forall a\\in F\\)</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#subspace","title":"subspace","text":"<p>A subset \\(\\mathsf{W}\\) of a vector space \\(\\mathsf{V}\\) over a field \\(F\\) is called a subspace of \\(\\mathsf{V}\\) if \\(\\mathsf{W}\\) is a vector space over \\(F\\) with the operations of addition and scalar multiplication defined on \\(\\mathsf{V}\\).</p> <p>In any vector space \\(\\mathsf{V}\\), note that \\(\\mathsf{V}\\) and \\(\\{0\\}\\) are subspaces. The latter is called the zero subspace of \\(\\mathsf{V}\\).</p> <p>Theorem 1.3(subspace):Let \\(\\mathsf{V}\\) be a vector space and \\(\\mathsf{W}\\) a subset of \\(\\mathsf{V}\\). Then \\(\\mathsf{W}\\) is a subspace of \\(\\mathsf{V}\\) iff the following three conditions hold for the operations defined in \\(\\mathsf{V}\\).</p> <ul> <li>\\(0 \\in \\mathsf{W}\\).</li> <li>\\(x+y \\in \\mathsf{W}\\) whenever \\(x \\in \\mathsf{W}\\) and \\(y \\in \\mathsf{W}\\).</li> <li>\\(cx \\in \\mathsf{W}\\) whenever \\(c\\in F\\) and \\(x \\in \\mathsf{W}\\).</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#examples","title":"Examples","text":""},{"location":"math/eng_method/linear_algebra1/#matrix_1","title":"matrix","text":"<p>The transpose \\(A^t\\) of an \\(m \\times n\\) matrix \\(A\\) is the \\(n \\times m\\) matrix obtained from \\(A\\) by interchanging the rows with the columns; that is, \\((A^t)_{ij} = A_{ji}\\).</p> <p>A symmetric matrix is a matrix \\(A\\) such that \\(A^t = A\\). </p> <ul> <li>Clearly, a symmetric matrix must be square. The set \\(W\\) of all symmetric matrices in \\(\\mathsf{M}_{nxn}(F)\\) is a subspace of \\(\\mathsf{M}_{nxn}(F)\\) since the conditions of Theorem 1.3 hold</li> </ul> <p>A diagonal matrix is a \\(n\\times n\\) matrix if \\(M_{ij}=0\\) whenever \\(i\\neq j\\) - the set of diagonal matrices is a subspace of \\(\\mathsf{M}_{nxn}(F)\\)</p>"},{"location":"math/eng_method/linear_algebra1/#polynominal_1","title":"polynominal","text":"<p>Let \\(n\\) be nonnegative integer, \\(\\mathsf{P}_n(F)\\) consists of all polynominals in \\(\\mathsf{P}(F)\\) having degree less than or equal to \\(n\\). - \\(\\mathsf{P}_n(F)\\) is a subspace of \\(\\mathsf{P}(F)\\)</p>"},{"location":"math/eng_method/linear_algebra1/#theorem-and-definition","title":"theorem and definition","text":"<p>Theorem1.4: Any intersecton of subspaces of a vector space \\(\\mathsf{V}\\) is a subspace of \\(\\mathsf{V}\\)</p> <p>sum of nonempty subsets \\(S_1\\) and \\(S_2\\) of a vector space \\(\\mathsf{V}\\) : - \\(S_1+S_2:=\\{x+y: x\\in S_1, y\\in S_2\\}\\)</p> <p>direct sum: \\(\\mathsf{W}_1\\oplus\\mathsf{W}_2\\)</p> <ul> <li>\\(\\mathsf{W}_1,\\mathsf{W}_2\\) are subspaces of \\(\\mathsf{V}\\)</li> <li>\\(\\mathsf{W}_1\\cap\\mathsf{W}_2=\\{0\\}, \\, \\mathsf{W}_1\\cup\\mathsf{W}_2=\\mathsf{V}\\)</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#14-linear-combination-and-systems-of-linear-equations","title":"1.4 linear combination and systems of linear equations","text":"<p>Definition: Let \\(\\mathsf{V}\\) be a vector space and \\(S\\) a nonempty subset of \\(\\mathsf{V}\\). A vector \\(\\vec{v} \\in \\mathsf{V}\\) is called a linear combination of vectors of \\(S\\) if there exists a finite number of vectors \\(\\vec{u}_1, \\vec{u}_2,.., \\vec{u}_n\\) in \\(S\\) and scalars \\(a_1, a_2,.., a_n\\) in \\(F\\) such that \\(\\vec{v}= a_1\\vec{u}_1+a_2\\vec{u}_2+...+ a_n\\vec{u}_n\\). In this case we also say that \\(\\vec{v}\\) is a linear combination of \\(\\vec{u}_1, \\vec{u}_2, \u00b7.., \\vec{u}_n\\) and call \\(a_1, a_2,\u00b7..\\), An the coefficients of the linear combination.</p> <p>Observe that in any vector space \\(\\mathsf{V}\\), \\(O\\vec{v} = \\vec{0}\\) for each \\(\\vec{v}\\in \\mathsf{V}\\). Thus the zero vector is a linear combination of any nonempty subset of \\(\\mathsf{V}\\).</p> <p>Definition. Let \\(s\\) be a nonempty subset of a vector space \\(\\mathsf{V}\\). The span of \\(S\\), denoted \\(\\operatorname{span}(S)\\), is the set consisting of all linear combinations of the vectors in \\(S\\). For convenience, we define \\(\\operatorname{span}() = \\{0\\}\\).</p> <p>In \\(\\mathbb{R}^3\\), for instance, the span of the set \\(\\{(1,0,0), (0,1,0)\\}\\) consists of al vectors in \\(R3\\) that have the form \\(a(1,0,0) + b(0,1,0) = (a,b,0)\\) for some scalars \\(a\\) and \\(b\\). Thus the span of \\(\\{(1,0,0), (0,1,  0)\\}\\) contains all the points in the xy-plane. In this case, the span of the set is a subspace of \\(\\mathbb{R}^3\\). This fact is true in general.</p> <p>Theorem 1.5(span and subspace):  - The span of any subset \\(S\\) of a vector space \\(\\mathsf{V}\\) is a subspace of \\(\\mathsf{V}\\).  - Any subspace of \\(\\mathsf{V}\\) that contains \\(S\\) must also contain the span of \\(S\\)</p> <p>Definition: A subset \\(S\\) of a vector space \\(\\mathsf{V}\\) generates (or spans)\\(\\mathsf{V}\\) if \\(\\operatorname{span}(S)=\\mathsf{V}\\). In this case, we also say that the vectors of \\(S\\) generate (or span)\\(\\mathsf{V}\\).</p>"},{"location":"math/eng_method/linear_algebra1/#15-linear-dependence-and-linear-independence","title":"1.5 linear dependence and linear independence","text":"<p>Definition: A subset \\(S\\) of a vector space \\(\\mathsf{V}\\) is called linearly dependent if there exists a finite number of distinct vectors \\(\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_n\\) in \\(S\\) and scalars \\(a_1,a_2,\\cdots ,a_n\\), not all zero, such that </p> \\[a_1\\vec{u}_1+a_2\\vec{u}_2+\\cdots+a_n\\vec{u}_n=\\vec{0}\\] <p>In this case we also say that the vectors of \\(S\\) are linearly dependent</p> <p>For any vectors \\(\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_n\\), We have \\(a_1\\vec{u}_1+a_2\\vec{u}_2+\\cdots+a_n\\vec{u}_n=\\vec{0}\\) if \\(a_1=a_2=\\cdots =a_n=0\\). We call this the trivial representation of \\(\\vec{0}\\) as a linear combination of \\(\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_n\\). Thus, for a set to be linearly dependent, there must exists a nontrivial representation of \\(\\vec{0}\\) as a linear combination of vectors in the set. </p> <p>Consequently, any subset of a vector space that contains the zero vector is linearly dependent because \\(\\vec{0} =1\\cdot \\vec{0}\\) is a nontrivial representation of 0 as a 1linear combination of vectors in the set.\\</p> <p>Definition: A subset \\(S\\) of a vector space that is not linearly dependent is called linearly independent. As before, we also say that the vectors ot \\(S\\) are linearly independent.</p> <p>The following facts about linearly independent sets are true in any vector space:</p> <ol> <li>The empty set is linearly independent, for linearly dependent sets must be nonempty.</li> <li>A set consisting of a single nonzero vector is linearly independent. For if \\(\\{\\vec{u}\\}\\) is linearly dependent. then \\(a\\vec{u}=0\\) for some nonzero scalar \\(a\\). Thus</li> </ol> \\[\\vec{u}=a^{-1}(a\\vec{u})=a^{-1}\\vec{0}=\\vec{0}\\] <ol> <li>A set is linearly independent iff the only representations of \\(\\vec{0}\\) as linear combinations of its vectors are trivial representations</li> </ol>"},{"location":"math/eng_method/linear_algebra1/#examples_1","title":"Examples","text":""},{"location":"math/eng_method/linear_algebra1/#polynominal_2","title":"polynominal","text":"<p>For \\(k=0,1,\\cdots, n\\), let \\(p_k(x)=x^k+x^{k+1}+\\cdots+x^n\\), The set  \\(\\(\\{p_0(x),p_1(x),\\cdots,p_n(x)\\}\\)\\)</p> <p>is linearly independent in \\(\\mathsf{P}_n(F)\\). </p>"},{"location":"math/eng_method/linear_algebra1/#theorem","title":"theorem","text":"<p>Theorem 1.6: Let \\(\\mathsf{V}\\) be a vector space, and let \\(S_1 \\subseteq S_2 \\subseteq \\mathsf{V}\\).If \\(S_1\\) is</p> <p>linearly dependent, then \\(S_2\\) is linearly dependent</p> <p>Corollary: Let \\(\\mathsf{V}\\) be a vector space,  \\(S_1 \\subseteq S_2 \\subseteq \\mathsf{V}\\). If \\(S_2\\) is linearly independent,  then \\(S_1\\) is linearly independent.</p> <p>Theorem 1.7: Let \\(S\\) be a linearly independent subset of a vector space \\(\\mathsf{V}\\).and let \\(\\vec{v}\\) be a vector in \\(\\mathsf{V}\\) that is not in \\(S\\). Then \\(S\\cup \\{\\vec{v}\\}\\) is linearly dependent if and only if \\(\\vec{v} \\in \\operatorname{span}(S)\\).</p>"},{"location":"math/eng_method/linear_algebra1/#16-bases-and-dimension","title":"1.6 bases and dimension","text":""},{"location":"math/eng_method/linear_algebra1/#basic","title":"basic","text":"<p>Definition: A basis \\(\\beta\\) for a vector space \\(\\mathsf{V}\\) is a linearly independent subset of \\(\\mathsf{V}\\) that generates \\(\\mathsf{V}\\). If \\(\\beta\\) is a basis for \\(\\mathsf{V}\\), we also say that the vectors of \\(\\beta\\) form a basis for V.</p>"},{"location":"math/eng_method/linear_algebra1/#examples_2","title":"Examples","text":"<ul> <li>Recalling that \\(\\operatorname{span}(\\phi) = \\{0\\}\\) and \\(\\phi\\) is linearly independent, we see that is a basis for the zero vector space.</li> <li>In \\(\\mathsf{F}^n\\), let \\(\\vec{e}_1 = (1,0,0,\\cdots, 0), \\vec{e}_2=(0,1,0,\\cdots,0),\\cdots,\\vec{e}_n=(0,0,\\cdots,0,1); \\{\\vec{e}_1,\\vec{e_2},\\cdots,\\vec{e}_n\\}\\) is readily seen to be a basis for \\(\\mathsf{F}^n\\) and is called the standard basis for \\(\\mathsf{F}^n\\).</li> <li>In \\(\\mathsf{M}_{m\\times n}(F)\\), let \\(E^{ij}\\) denote the matrix whose only nonzero entry is a \\(1\\) in the ith row and jth column. Then \\({E^{ij}: 1\\le i \\le m, 1 \\le j \\le n}\\) is a basis for \\(\\mathsf{M}_{m\\times n}(F)\\).</li> <li>In \\(\\mathsf{P}_n (F)\\) the set \\(\\{1, x, x^2,\\cdots, x^n\\}\\) is a basis. We call this basis the standard basis for \\(\\mathsf{P}_n (F)\\).</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#theorem_1","title":"Theorem","text":"<p>Theorem 1.8: Let \\(\\mathsf{V}\\) be a vector space and \\(\\beta=\\{\\vec{u}_1, \\vec{u}_2,\\cdot, \\vec{u}_n\\}\\) be a subset of \\(\\mathsf{V}\\). Then \\(\\beta\\) is a basis for \\(\\mathsf{V}\\) if and only if each \\(\\vec{v}\\in\\mathsf{V}\\) can be uniquely expressed as a linear combination of vectors of \\(\\beta\\), that is, can be expressed in the form</p> \\[\\vec{v} = a_1\\vec{u}_1 + a_2\\vec{u}_2 + \\cdots + a_n \\vec{u}_n\\] <p>for unique scalars \\(a_1, a_2,\\cdots, a_n\\).</p> <p>Theorem 1.9: If a vector space \\(\\mathsf{V}\\) is generated by a finite set \\(S\\), then some subset of \\(S\\) is a basis for \\(\\mathsf{V}\\). Hence \\(\\mathsf{V}\\) has a finite basis</p> <p>Theorem 1.10 (Replacement Theorem): Let \\(\\mathsf{V}\\) be a vector space that is generated by a set \\(G\\) containing exactly \\(n\\) vectors, and let \\(L\\) be a linearly independent subset of \\(\\mathsf{V}\\) containing exactly \\(m\\) vectors. Then \\(m\\le n\\) and there exists a subset \\(H\\) of \\(G\\) containing exactly \\(n \u2013 m\\) vectors such that \\(L\\cup H\\) generates \\(\\mathsf{V}\\).</p> <p>Corollary 1: Let \\(\\mathsf{V}\\) be a vector space having a finite basis. Then every basis for \\(\\mathsf{V}\\) contains the same number of vectors</p>"},{"location":"math/eng_method/linear_algebra1/#dimension","title":"dimension","text":"<p>Definitions: A vector space is called finite-dimensional if it has a basis consisting of a finite number of vectors. The unique number of vectors in each basis for \\(\\mathsf{V}\\) is called the dimension of \\(\\mathsf{V}\\) and is denoted by \\(\\operatorname{dim}(\\mathsf{V})\\). A vector space that is not finite-dimensional is called infinite-dimensional. </p>"},{"location":"math/eng_method/linear_algebra1/#example","title":"Example","text":"<ul> <li>The vector space \\(\\{0\\}\\) has dimension \\(0\\)</li> <li>The vector space \\(\\mathsf{F}^n\\) has dimension \\(n\\) </li> <li>The vector space \\(\\mathsf{M}_{m\\times n}(F)\\) has dimension \\(mn\\)</li> <li>The vector space \\(\\mathsf{P}_n(F)\\) has dimension \\(n+1\\) </li> </ul> <p>The following examples show that the dimension of a vector space depends on its field of scalars. </p> <ul> <li>Over the field of complex numbers, the vector space of complex numbers has dimension \\(1\\). (A basis is \\(\\{1\\}\\).) </li> <li>Over the field of real numbers, the vector space of complex numbers has dimension \\(2\\). (A basis is \\(\\{1, i\\}\\))</li> </ul>"},{"location":"math/eng_method/linear_algebra1/#corollary-and-theorem","title":"Corollary and theorem","text":"<p>Corollary 2: Let \\(\\mathsf{V}\\) be a vector space with dimension \\(n\\). </p> <ol> <li>Any finite generating set for \\(\\mathsf{V}\\) contains at least \\(n\\) vectors, and a generating set for \\(\\mathsf{V}\\) that contains exactly \\(n\\) vectors is a basis for \\(\\mathsf{V}\\). </li> <li>Any linearly independent subset of \\(\\mathsf{V}\\) that contains exactly is vectors is a basis for \\(\\mathsf{V}\\). </li> <li>Every linearly independent subset of \\(\\mathsf{V}\\) can be extended to a basis for \\(\\mathsf{V}\\). </li> </ol> <p>Theorem 1.11: Let \\(\\mathsf{W}\\) be a subspace of a finite-dimensional vector space \\(\\mathsf{V}\\). Then \\(\\mathsf{W}\\) is finite-dimensional and \\(\\operatorname{dim}(\\mathsf{W}) &lt; \\operatorname{dim}(\\mathsf{V})\\). Moreover, if \\(\\operatorname{dim}(\\mathsf{W}) = \\operatorname{dim}(\\mathsf{V})\\), then \\(\\mathsf{V} = \\mathsf{W}\\).</p> <p>The set of diagonal \\(n\\times n\\) matrices is a subspace \\(\\mathsf{W}\\) of \\(\\mathsf{M}_{n\\times n} (F)\\). A basis for \\(\\mathsf{W}\\) is</p> \\[{E^{11}, E^{22},\\cdots,E^{nn}},\\] <p>where \\(E^{ij}\\) is the matrix in which the only nonzero entry is a 1 in the ith row and jth column. Thus \\(\\operatorname{dim}(\\mathsf{W})= n\\).</p> <p>Corollary: If \\(\\mathsf{W}\\) is a subspace of a finite-dimensional vector space \\(\\mathsf{V}\\), then any basis for \\(\\mathsf{W}\\) can be extended to a basis for \\(\\mathsf{V}\\).</p>"}]}